{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import glob as gb\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get training data\n",
    "img_trainpath = gb.glob(\"training//*.jpg\")\n",
    "train_x = []\n",
    "train_y = []\n",
    "for path in img_trainpath:\n",
    "    pic = Image.open(path)\n",
    "    pix = np.array(pic)\n",
    "    train_x.append(pix)\n",
    "    tmp = str(path)\n",
    "    a = tmp.split('/')[1].split('_')[0]\n",
    "    a = int(a)\n",
    "    onehot_tmp = np.zeros(11)\n",
    "    onehot_tmp[a] = 1\n",
    "    onehot_tmp = np.array(onehot_tmp)\n",
    "    train_y.append(onehot_tmp)\n",
    "\n",
    "train_y = np.array(train_y)\n",
    "train_x = np.array(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_x.shape)\n",
    "print(img_trainpath[0])\n",
    "print(train_y[0])\n",
    "#tmp = gb.glob('training//*.jpg')\n",
    "#print(tmp[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get test data\n",
    "img_testpath = gb.glob(\"evaluation//*.jpg\")\n",
    "test_x = []\n",
    "test_y = []\n",
    "for path in img_testpath:\n",
    "    pic = Image.open(path)\n",
    "    pix = np.array(pic)\n",
    "    #pix = np.array(pic.getdata()).reshape(pic.size[0], pic.size[1], 3)\n",
    "    test_x.append(pix)\n",
    "    tmp = str(path)\n",
    "    a = tmp.split('/')[1].split('_')[0]\n",
    "    a = int(a)\n",
    "    onehot_tmp = np.zeros(11)\n",
    "    onehot_tmp[a] = 1\n",
    "    onehot_tmp = np.array(onehot_tmp)\n",
    "    test_y.append(onehot_tmp)\n",
    "test_y = np.array(test_y)\n",
    "test_x = np.array(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_x.shape)\n",
    "print(img_testpath[0])\n",
    "print(test_y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(v_xs, v_ys):\n",
    "    global prediction\n",
    "    y_pre = sess.run(prediction, feed_dict={xs: v_xs, keep_prob: 1})\n",
    "    correct_prediction = tf.equal(tf.argmax(y_pre,1), tf.argmax(v_ys,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    result = sess.run(accuracy, feed_dict={xs: v_xs, ys: v_ys, keep_prob: 1})\n",
    "    return result\n",
    "\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape,  mean=0, stddev=0.05)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.truncated_normal(shape=shape,mean=0, stddev=0.05)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W):\n",
    "    # stride [1, x_movement, y_movement, 1]\n",
    "    # Must have strides[0] = strides[3] = 1\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    # stride [1, x_movement, y_movement, 1]\n",
    "    return tf.nn.max_pool(x, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# placeholder\n",
    "xs = tf.placeholder(tf.float32, [None, 96,96,3])/255.   \n",
    "ys = tf.placeholder(tf.float32, [None, 11])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "print(xs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## conv1,max1 layer ##\n",
    "W_conv1 = weight_variable([3,3,3,32]) # patch 3x3, in size 3, out size 32\n",
    "b_conv1 = bias_variable([32])\n",
    "h_conv1 = tf.nn.selu(conv2d(xs, W_conv1) + b_conv1)  \n",
    "h_pool1 = max_pool_2x2(h_conv1)                      #### output size 48x48x32\n",
    "\n",
    "## conv2,max2 layer ##\n",
    "W_conv2 = weight_variable([3,3,32,64]) # patch 3x3, in size 32, out size 64\n",
    "b_conv2 = bias_variable([64])\n",
    "h_conv2 = tf.nn.selu(conv2d(h_pool1, W_conv2) + b_conv2)  \n",
    "h_pool2 = max_pool_2x2(h_conv2)                      #### output size 24x24x64\n",
    "\n",
    "## conv3,max3 layer ##\n",
    "W_conv3 = weight_variable([3,3,64,128]) # patch 3x3, in size 64, out size 128\n",
    "b_conv3 = bias_variable([128])\n",
    "h_conv3 = tf.nn.selu(conv2d(h_pool2, W_conv3) + b_conv3)  \n",
    "h_pool3 = max_pool_2x2(h_conv3)                      #### output size 12x12x128\n",
    "\n",
    "## conv4,max4 layer ##\n",
    "W_conv4 = weight_variable([3,3,128,256]) # patch 3x3, in size 128, out size 256\n",
    "b_conv4 = bias_variable([256])\n",
    "h_conv4 = tf.nn.selu(conv2d(h_pool3, W_conv4) + b_conv4)  \n",
    "h_pool4 = max_pool_2x2(h_conv4)                      #### output size 6x6x256\n",
    "\n",
    "## fc1 layer ##\n",
    "W_fc1 = weight_variable([6*6*256, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "# [n_samples, 6, 6, 256] ->> [n_samples, 6*6*256]\n",
    "h_pool4_flat = tf.reshape(h_pool4, [-1, 6*6*256])\n",
    "h_fc1 = tf.nn.selu(tf.matmul(h_pool4_flat, W_fc1) + b_fc1)\n",
    "\n",
    "## fc2 layer ##\n",
    "W_fc2 = weight_variable([1024, 1024])\n",
    "b_fc2 = bias_variable([1024])\n",
    "h_fc2 = tf.nn.selu(tf.matmul(h_fc1, W_fc2) + b_fc2)\n",
    "h_fc2_drop = tf.nn.dropout(h_fc2, keep_prob)\n",
    "\n",
    "W_fc3 = weight_variable([1024, 11])\n",
    "b_fc3 = bias_variable([11])\n",
    "prediction = tf.matmul(h_fc2_drop, W_fc3) + b_fc3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss\n",
    "value = tf.trainable_variables()\n",
    "l2_loss = 0.\n",
    "lamda = 1\n",
    "for i in value:\n",
    "    l2_loss += tf.norm(i)\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=\n",
    "                                                        prediction, labels=ys)) + lamda * l2_loss\n",
    "# cross_entropy = tf.reduce_mean(-tf.reduce_sum(ys * tf.log(prediction) + (1-ys) * tf.log(1-prediction),\n",
    "#                                              reduction_indices=[1]))\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "\n",
    "# training\n",
    "total_train = train_x.shape[0]\n",
    "total_test = test_x.shape[0]\n",
    "batch_size = 64\n",
    "test_batch_size = 64\n",
    "iteration = 50\n",
    "train_accuracy = []\n",
    "test_accuracy = []\n",
    "loss_ = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# important step initial all the variables\n",
    "sess = tf.Session()\n",
    "if int((tf.__version__).split('.')[1]) < 12 and int((tf.__version__).split('.')[0]) < 1:\n",
    "    init = tf.initialize_all_variables()\n",
    "else:\n",
    "    init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(iteration):\n",
    "    #training\n",
    "    for j in range(total_train % batch_size - 1):\n",
    "        tmp_x = train_x[j*batch_size:j*batch_size+batch_size,:,:,:]\n",
    "        tmp_y = train_y[j*batch_size:j*batch_size+batch_size,:]\n",
    "        sess.run(train_step, feed_dict={xs: tmp_x, ys: tmp_y, keep_prob: 0.6})\n",
    "    loss_tmp = []\n",
    "    train_acc = []\n",
    "    test_acc = []\n",
    "    # calculate training_accuracy and loss\n",
    "    for m in range(total_train % test_batch_size - 1):\n",
    "        tmp_x = train_x[m*test_batch_size:m*test_batch_size+test_batch_size,:,:,:]\n",
    "        tmp_y = train_y[m*test_batch_size:m*test_batch_size+test_batch_size,:]\n",
    "        loss_tmp.append(sess.run(cross_entropy,feed_dict={xs: tmp_x, \n",
    "                                                     ys: tmp_y, keep_prob: 1}))\n",
    "        train_acc.append(compute_accuracy(tmp_x, tmp_y))\n",
    "    #calculate test_accuracy    \n",
    "    for n in range(total_test % test_batch_size - 1):\n",
    "        tmp_x = test_x[n*test_batch_size:n*test_batch_size+test_batch_size,:,:,:]\n",
    "        tmp_y = test_y[n*test_batch_size:n*test_batch_size+test_batch_size,:]\n",
    "        test_acc.append(compute_accuracy(tmp_x, tmp_y))\n",
    "    train_tmp_acc = np.sum(train_acc)/(m + 1)\n",
    "    test_tmp_acc = np.sum(test_acc)/(n + 1)\n",
    "    loss_tmp_ = np.sum(loss_tmp)/(m + 1)\n",
    "    loss_.append(loss_tmp_)\n",
    "    train_accuracy.append(train_tmp_acc)\n",
    "    test_accuracy.append(test_tmp_acc)\n",
    "    print('iteration:',i,'train_accuracy:',train_tmp_acc,'test_accuracy:',test_tmp_acc)\n",
    "    print('loss:',loss_tmp_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.arange(50)\n",
    "plt.plot(t,loss_)\n",
    "plt.title('Learing Curve with L2 norm')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Cross Entropy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 =plt.plot(t,train_accuracy,'r', label='training_accuracy')\n",
    "p2 = plt.plot(t,test_accuracy,'b', label='test_accuracy')\n",
    "plt.title('training and test accuray with L2 norm')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = W_conv1.eval(session=sess)\n",
    "binwidth = 0.001\n",
    "a = a.flatten()\n",
    "a.shape\n",
    "plt.hist(a,bins = 50)  # arguments are passed to np.histogram\n",
    "plt.title(\"Histogram of conv1\")\n",
    "plt.show()\n",
    "a = W_conv2.eval(session=sess)\n",
    "a = a.flatten()\n",
    "a.shape\n",
    "plt.hist(a,bins = 50)  # arguments are passed to np.histogram\n",
    "plt.title(\"Histogram of conv2\")\n",
    "plt.show()\n",
    "a = W_conv3.eval(session=sess)\n",
    "a = a.flatten()\n",
    "a.shape\n",
    "plt.hist(a,bins = 50)  # arguments are passed to np.histogram\n",
    "plt.title(\"Histogram of conv3\")\n",
    "plt.show()\n",
    "a = W_conv4.eval(session=sess)\n",
    "a = a.flatten()\n",
    "a.shape\n",
    "plt.hist(a,bins = 50)  # arguments are passed to np.histogram\n",
    "plt.title(\"Histogram of conv4\")\n",
    "plt.show()\n",
    "a = W_fc1.eval(session=sess)\n",
    "a = a.flatten()\n",
    "a.shape\n",
    "plt.hist(a,bins = 50)  # arguments are passed to np.histogram\n",
    "plt.hist(a,bins = 50)\n",
    "plt.title(\"Histogram of fc1\")\n",
    "plt.show()\n",
    "a = W_fc2.eval(session=sess)\n",
    "a = a.flatten()\n",
    "a.shape\n",
    "plt.hist(a,bins = 50)  # arguments are passed to np.histogram\n",
    "plt.title(\"Histogram of fc2\")\n",
    "plt.show()\n",
    "a = W_fc3.eval(session=sess)\n",
    "a = a.flatten()\n",
    "a.shape\n",
    "plt.hist(a,bins = 50)  # arguments are passed to np.histogram\n",
    "plt.title(\"Histogram of fc3\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.image as mpimg\n",
    "a1 = sess.run(h_pool1,feed_dict={xs: train_x[3500:3501,:,:,:], keep_prob: 1})\n",
    "\n",
    "img = train_x[500,:,:,:]\n",
    "a1 = np.array(a1).reshape([48,48,32])\n",
    "print(a1.shape)\n",
    "print(train_y[3500,:])\n",
    "#img = a1[:,:,16]\n",
    "imgplot = plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.norm(W_fc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow 1.8",
   "language": "python",
   "name": "tf18"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
